{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Linear regression with gradient descent}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple gradient descent \n",
    "# Input: Df=gradient, x0=initial guess, N=maximal number of steps, eps=tolerance\n",
    "# Output: x=global minimum, steps=number of steps, norm of the gradient\n",
    "def GD(Df, x0, eta, N, eps):\n",
    "    x=x0\n",
    "    Gradf_norm =  np.dot(Df(x0),Df(x0))**0.5 \n",
    "    steps = 0\n",
    "    while abs(Gradf_norm) > eps and steps < N:\n",
    "        #print(x)\n",
    "        Df_value=Df(x)\n",
    "        Gradf_norm =  np.dot(Df_value,Df_value)**0.5\n",
    "        x = x-eta*Df_value\n",
    "        #print('Gradf_norm =',Gradf_norm)\n",
    "        steps = steps + 1\n",
    "    # Either a solution is found, or too many iterations\n",
    "    if abs(Gradf_norm) > eps:\n",
    "        steps = -1\n",
    "    return x, steps, Gradf_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(t, x):\n",
    "    return x[0]+t*x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Least square function \n",
    "def MLSE(x, t, y):\n",
    "    m = y.size\n",
    "    return (1/m)*np.dot(linear_model(t, x)-y,linear_model(t, x)-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of least square function \n",
    "def dMLSE(x, t, y):\n",
    "    m = y.size\n",
    "    return (1/m)*np.array([2*np.dot((linear_model(t, x)-y),np.ones(m)),\n",
    "                    2*np.dot((linear_model(t, x)-y),t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return MLSE(x,td,yd)\n",
    "def df(x):\n",
    "    return dMLSE(x,td,yd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.74409975,  3.15082899]), 367, 0.0009835324664540243)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GD(df, x0=np.array([10,10]), eta=.06, N=1000, eps=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtracking function\n",
    "#Input: Df=gradient, x0=initial guess,\n",
    "#        hyperparameters eta=intial stepsize,  factors alpha, beta, \n",
    "#        e.g., eta=1, alpha=0.5, beta=0.8\n",
    "#        cout=maximal number of steps \n",
    "# Output: eta=adjusted step size\n",
    "def backtrack(f,Df,x0, eta, alpha, beta, count):\n",
    "    while ((f(x0) - (f(x0 - eta*df(x0)) + alpha * eta * np.dot(df(x0),df(x0))))< 0):\n",
    "        eta = eta*beta\n",
    "        #print(\"Iteration\", cout,\"Inequality: \",  f(x0) \\\n",
    "        #      -(f(x0 - eta*df(x0))+ alpha * eta * np.dot(df(x0),df(x0))))\n",
    "        count=count+ 1      \n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent with backtracking\n",
    "# Input: Df=gradient, x0=initial guess, \n",
    "#        hyperparameters eta=intial stepsize,  factors alpha, beta, \n",
    "#        e.g., eta=1, alpha=0.5, beta=0.8\n",
    "#        N=maximal number of steps, eps=tolerance\n",
    "# Output: x=global minimum, steps=number of steps, norm of the gradient\n",
    "def GD_bt(f,Df, x0, eta, alpha, beta, N, eps):\n",
    "    x=x0\n",
    "    Gradf_norm =  (np.dot(Df(x0),Df(x0)))**0.5\n",
    "    steps = 0\n",
    "    while abs(Gradf_norm) > eps and steps < N:      \n",
    "        #Condition to adjust the step-size comment out to omit backtracking\n",
    "        eta=backtrack(f,Df, x0, eta, alpha, beta, N)\n",
    "        x = x-eta*Df(x)\n",
    "        Gradf_norm =  (np.dot(Df(x),Df(x)))**0.5\n",
    "        steps = steps + 1 \n",
    "    print('Final eta=',eta)\n",
    "    # Either a solution is found, or too many iterations\n",
    "    if abs(Gradf_norm) > eps:\n",
    "        steps = -1\n",
    "    return x, steps, Gradf_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eta= 0.054975581388800036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.74405168,  3.15081776]), 400, 0.0009800105037305299)"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GD_bt(f,df, x0=np.array([10,10]), eta=1.0, alpha=0.1, beta=0.8, N=1000, eps=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
