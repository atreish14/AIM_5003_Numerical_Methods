{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Gradient descent with backtracking}$\n",
    "\n",
    "The code is applying the gradient descent method with backtracking \n",
    "to find the global minimum for a given function $f(x)$.\n",
    "Backtracking refers to adjusting the stepsize of the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtracking function\n",
    "#Input: Df=gradient, x0=initial guess,\n",
    "#        hyperparameters eta=intial stepsize,  factors alpha, beta, \n",
    "#        e.g., eta=1, alpha=0.5, beta=0.8\n",
    "#        cout=maximal number of steps \n",
    "# Output: eta=adjusted step size\n",
    "def backtrack(f,Df,x0, eta, alpha, beta, count):\n",
    "    while (f(x0) - (f(x0 - eta*df(x0)) + alpha * eta * np.dot(df(x0),df(x0)))< 0):\n",
    "        eta = eta*beta\n",
    "        #print(\"Iteration\", cout,\"Inequality: \",  f(x0) \\\n",
    "        #      -(f(x0 - eta*df(x0))+ alpha * eta * np.dot(df(x0),df(x0))))\n",
    "        count=count+ 1      \n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent with backtracking\n",
    "# Input: Df=gradient, x0=initial guess, \n",
    "#        hyperparameters eta=intial stepsize,  factors alpha, beta, \n",
    "#        e.g., eta=1, alpha=0.5, beta=0.8\n",
    "#        N=maximal number of steps, eps=tolerance\n",
    "# Output: x=global minimum, steps=number of steps, norm of the gradient\n",
    "def GD_bt(f,Df, x0, eta, alpha, beta, N, eps):\n",
    "    x=x0\n",
    "    Gradf_norm =  (np.dot(Df(x0),Df(x0)))**0.5\n",
    "    steps = 0\n",
    "    while abs(Gradf_norm) > eps and steps < N:      \n",
    "        #Condition to adjust the step-size comment out to omit backtracking\n",
    "        eta=backtrack(f,Df, x0, eta, alpha, beta, N)\n",
    "        x = x-eta*Df(x)\n",
    "        Gradf_norm =  (np.dot(Df(x),Df(x)))**0.5\n",
    "        steps = steps + 1 \n",
    "    print('Final eta=',eta)\n",
    "    # Either a solution is found, or too many iterations\n",
    "    if abs(Gradf_norm) > eps:\n",
    "        steps = -1\n",
    "    return x, steps, Gradf_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    return 0.5*(X[0]**2+10*X[1]**2)\n",
    "def df(X):\n",
    "    return np.array([X[0],10*X[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eta= 0.1677721600000001\n",
      "minimum= [9.57893243e-07 1.35680307e-15] steps= 88 norm of gradient= 9.578932428967748e-07\n"
     ]
    }
   ],
   "source": [
    "X0=np.array([10.,1.])\n",
    "x, steps, Gradf_norm=GD_bt(f,df,X0,eta=1.0,alpha=0.5,beta=0.8, N=100,eps=1.e-6)\n",
    "print('minimum=',x,'steps=',steps,'norm of gradient=', Gradf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    return ((3/4.)*X[0]-(3/2.)*X[1])**2+(X[1]-2.)**2+(1/4.)*X[0]*X[1]\n",
    "def df(X):\n",
    "    return np.array([(9/8.)*X[0]-(9/4.)+(1/4.)*X[1],2*X[1]-4+(1/4.)*X[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eta= 0.5120000000000001\n",
      "minimum= [1.60000043 1.79999988] steps= 20 norm of gradient= 4.7500167798537444e-07\n"
     ]
    }
   ],
   "source": [
    "X0=np.array([5.,4.])\n",
    "x, steps, Gradf_norm=GD_bt(f,df,X0,eta=1.0,alpha=0.5,beta=0.8,N=100,eps=1.e-6)\n",
    "print('minimum=',x,'steps=',steps,'norm of gradient=', Gradf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    return np.exp(X[0]+3*X[1]-0.1)+np.exp(X[0]-3*X[1]-0.1) + np.exp(-X[0]-0.1)\n",
    "def df(X):\n",
    "    return np.array([np.exp(X[0]+3*X[1]-0.1) + np.exp(X[0]-3*X[1]-0.1) - np.exp(-X[0]-0.1), 3*np.exp(X[0]+3*X[1]-0.1) -3* np.exp(X[0]-3*X[1]-0.1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eta= 0.04035360699999998\n",
      "minimum= [-3.46214564e-01 -6.88035978e-18] steps= 68 norm of gradient= 0.0009188439568874607\n"
     ]
    }
   ],
   "source": [
    "X0=np.array([0.5,0.5])\n",
    "x, steps, Gradf_norm=GD_bt(f,df,X0,eta=1.0,alpha=0.1,beta=0.7,N=200,eps=1.e-3)\n",
    "print('minimum=',x,'steps=',steps,'norm of gradient=', Gradf_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact solution= [-0.34657359  0.        ]\n"
     ]
    }
   ],
   "source": [
    "Xexact=np.array([-np.log(2)/2,0])\n",
    "print('Exact solution=',Xexact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
